@article{tran-2021,
author = {Tran, Minh-Ngoc and Nguyen, Trong-Nghia and Dao, Hung},
year = {2020},
month = {03},
pages = {},
title = {A practical tutorial on Variational Bayes}
}
@article{pollock-2020,
	author = {given-i=M., given=Murray, family=Pollock and given-i=P., given=Paul, family=Fearnhead and given-i={A. M.}, given={Adam M.}, family=Johansen and given-i={G. O.}, given={Gareth O.}, family=Roberts},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	month = {10},
	number = {5},
	pages = {1167--1221},
	publisher = {Wiley},
	title = {{Quasi‐stationary Monte Carlo and the ScaLE algorithm}},
	volume = {82},
	year = {2020},
	doi = {10.1111/rssb.12365},
	url = {http://dx.doi.org/10.1111/rssb.12365},
}
@book{kingman-1993,
	author = {given-i={J.F. C.}, given=J., family=Kingman},
	edition = {1},
	month = {1},
	publisher = {Clarendon Press},
	title = {{Poisson Processes (Oxford Studies in Probability, 3)}},
	year = {1993},
}
@article{pollock-2016,
	author = {given-i=M., given=Murray, family=Pollock and given-i={A. M.}, given={Adam M.}, family=Johansen and given-i={G. O.}, given={Gareth O.}, family=Roberts},
	journal = {Bernoulli},
	month = {5},
	number = {2},
	publisher = {Bernoulli Society for Mathematical Statistics and Probability},
	title = {{On the exact and $\varepsilon$-strong simulation of (jump) diffusions}},
	volume = {22},
	year = {2016},
	doi = {10.3150/14-bej676},
	url = {http://dx.doi.org/10.3150/14-bej676},
}
@article{WANG20203193,
title = {An approximation scheme for quasi-stationary distributions of killed diffusions},
journal = {Stochastic Processes and their Applications},
volume = {130},
number = {5},
pages = {3193-3219},
year = {2020},
issn = {0304-4149},
doi = {https://doi.org/10.1016/j.spa.2019.09.010},
url = {https://www.sciencedirect.com/science/article/pii/S0304414918304332},
author = {Andi Q. Wang and Gareth O. Roberts and David Steinsaltz},
keywords = {Asymptotic pseudo-trajectory, Killed diffusion, Quasi-stationary distribution, Quasi-stationary Monte Carlo method, Stochastic approximation},
abstract = {In this paper we study the asymptotic behavior of the normalized weighted empirical occupation measures of a diffusion process on a compact manifold which is killed at a smooth rate and then regenerated at a random location, distributed according to the weighted empirical occupation measure. We show that the weighted occupation measures almost surely comprise an asymptotic pseudo-trajectory for a certain deterministic measure-valued semiflow, after suitably rescaling the time, and that with probability one they converge to the quasi-stationary distribution of the killed diffusion. These results provide theoretical justification for a scalable quasi-stationary Monte Carlo method for sampling from Bayesian posterior distributions.}
}
@book{Pai-2018,
author={Pai,Praseed and Abraham,Peter},
year={2018},
title={C++ reactive programming: design concurrent and asynchronous applications using the RxCpp library and modern C++ 17},
publisher={Packt},
address={Mumbai;Birmingham, England;},
keywords={Application software; Development},
isbn={9781788624244;1788624246;},
language={English},
}
@book{lippman-2012,
	author = {given-i=S., given=Stanley, family=Lippman and given-i=J., given=Josée, family=Lajoie and given-i=B., given=Barbara, family=Moo},
	date = {2012-08-06},
	edition = {5},
	language = {english},
	publisher = {Addison-Wesley Professional},
	title = {C++ Primer (5th Edition)},
}
@data{DVNHG7NV72008,
author = {},
publisher = {Harvard Dataverse},
title = {{Data Expo 2009: Airline on time data}},
year = {2008},
version = {V1},
doi = {10.7910/DVN/HG7NV7},
url = {https://doi.org/10.7910/DVN/HG7NV7}
}
@unpublished{kumar2018,
           month = {September},
           title = {On a quasi-stationary approach to bayesian computation, with application to tall data},
          school = {University of Warwick},
          author = {Divakar Kumar},
            year = {2018},
             url = {https://wrap.warwick.ac.uk/132747/},
        abstract = {Markov Chain Monte Carlo (MCMC) techniques have traditionally been used in a Bayesian inference to simulate from an intractable distribution of parameters. However, the current age of Big data demands more scalable and robust algorithms for the inferences to be computationally feasible. Existing MCMC-based scalable methodologies often uses discretization within their construction and hence they are inexact. A newly proposed field of the Quasi-Stationary Monte Carlo (QSMC) methodology has paved the way for a scalable Bayesian inference in a Big data setting, at the same time, its exactness remains intact. Contrary to MCMC, a QSMC method constructs a Markov process whose quasi-stationary distribution is given by the target. A recently proposed QSMC method called the Scalable Langevin Exact (ScaLE) algorithm has been constructed by suitably combining the exact method of diffusion, the Sequential Monte Carlo methodology for quasi-stationarity and sub-sampling ideas to produce a sub-linear cost in a Big data setting. This thesis uses the mathematical foundations of the ScaLE methodology as a building block and carefully combines a recently proposed regenerative mechanism for quasistationarity to produce a new class of QSMC algorithm called the Regenerating ScaLE (ReScaLE). Further, it provides various empirical results towards the sublinear scalability of ReScaLE and illustrates its application to a real world big data problem where a traditional MCMC method is likely to suffer from a huge computational cost. This work takes further inroads into some current limitations faced by ReScaLE and proposes various algorithmic modifications for targeting quasistationarity. The empirical evidences suggests that these modifications reduce the computational cost and improve the speed of convergence.}
}
@Inbook{Chopin2020,
author="Chopin, Nicolas
and Papaspiliopoulos, Omiros",
title="Importance Resampling",
bookTitle="An Introduction to Sequential Monte Carlo",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="105--127",
abstract="ResamplingResamplingis the action of drawing randomly from a weighted sample, so as to obtain an unweighted sample. Resampling may be viewed as a random weight importance sampling technique. However it deserves a separate chapter because it plays a central role in particle filtering. In particular, we explain that resampling has the curious property of potentially reducing the variance at a later stage, provided that this later stage corresponds to a Markov update that forgets its past in some way. This point is crucial for the good performance of particle algorithms.",
isbn="978-3-030-47845-2",
doi="10.1007/978-3-030-47845-2_9",
url="https://doi.org/10.1007/978-3-030-47845-2_9"
}
@article{KUPTAMETEE2022110836,
title = {A review of resampling techniques in particle filtering framework},
journal = {Measurement},
volume = {193},
pages = {110836},
year = {2022},
issn = {0263-2241},
doi = {https://doi.org/10.1016/j.measurement.2022.110836},
url = {https://www.sciencedirect.com/science/article/pii/S0263224122001312},
author = {Chanin Kuptametee and Nattapol Aunsri},
keywords = {Particle degeneracy, Particle filter, Particle impoverishment, Resampling, Sequential Bayesian filtering, Genetic algorithm, Signal processing},
abstract = {A particle filtering (PF) is a sequential Bayesian filtering method suitable for non-linear non-Gaussian systems, which is widely used to estimate the states of parameters of interest that cannot be obtained directly but still relate to noisy measured data with probability masses. Possible values of targeted parameters (or particles) are sampled according to the related prior knowledge, with their probabilities (or weights) evaluated from the likelihood of being the true values of those parameters. However, most have negligible weights. The standard PF algorithm consists of three steps as particle generation, weight calculation or updating and particle regeneration, which is called resampling. The performance of PF depends greatly on the quality of particle regeneration. Resampling preserves and replicates particles with high weights, while those with low weights are eliminated. However, particle impoverishment is a side effect that reduces the diversity of particles used in the next time steps. Therefore, efficient resampling have to guarantee high likelihoods particles. This paper reviews the classification and qualitative descriptions of recent efficient particle weight-based resampling schemes and discusses their characteristics, implementations, advantages and disadvantages of each scheme.}
}
@misc{li2015preconditioned,
      title={Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks},
      author={Chunyuan Li and Changyou Chen and David Carlson and Lawrence Carin},
      year={2015},
      eprint={1512.07666},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}
@misc{RDocumentationFamily, url={https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/family}, journal={RDocumentation}}
@misc{Wickham, title={Advanced R}, url={https://adv-r.hadley.nz/r6.html}, journal={14 R6 | Advanced R}, author={Wickham, Hadley}}
@book{Dunn_Smyth_2018, place={New York, NY, U.S.A.}, title={Generalized linear models with examples in R}, publisher={Springer}, author={Dunn, Peter K. and Smyth, Gordon K.}, year={2018}}
@misc{Bryan, title={R packages (2E)}, url={https://r-pkgs.org/}, journal={R Packages (2e)}, author={Bryan, Hadley Wickham and Jennifer}}
@ARTICLE{1987PhLB..195..216D,
       author = {{Duane}, Simon and {Kennedy}, A.~D. and {Pendleton}, Brian J. and {Roweth}, Duncan},
        title = "{Hybrid Monte Carlo}",
      journal = {Physics Letters B},
         year = 1987,
        month = sep,
       volume = {195},
       number = {2},
        pages = {216-222},
          doi = {10.1016/0370-2693(87)91197-X},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1987PhLB..195..216D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{HMC,
       author = {{Duane}, Simon and {Kennedy}, A.~D. and {Pendleton}, Brian J. and {Roweth}, Duncan},
        title = "{Hybrid Monte Carlo}",
      journal = {Physics Letters B},
         year = 1987,
        month = sep,
       volume = {195},
       number = {2},
        pages = {216-222},
          doi = {10.1016/0370-2693(87)91197-X},
       adsurl = {https://ui.adsabs.harvard.edu/abs/1987PhLB..195..216D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{MALA,

author={Grenander,Ulf and Miller,Michael I.},

year={1994},

title={Representations of Knowledge in Complex Systems},

journal={Journal of the Royal Statistical Society. Series B, Methodological},

volume={56},

number={4},

pages={549-603},

abstract={Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject-matter knowledge that can be used as a basis for algorithmic `understanding' of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15 000 ×) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump-diffusion equation of generalized Langevin form. The jumps occur for the creation-annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. To justify this it is shown that the resulting jump-diffusion process is ergodic so that the solution converges to the desired probability measure. To speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.;SUMMARY

Modern sensor technologies, especially in biomedicine, produce increasingly detailed and informative image ensembles, many extremely complex. It will be argued that pattern theory can supply mathematical representations of subject‐matter knowledge that can be used as a basis for algorithmic ‘understanding’ of such pictures. After a brief survey of the basic principles of pattern theory we shall illustrate them by an application to a concrete situation: high magnification (greater than 15000×) electron micrographs of cardiac muscle cells. The aim is to build algorithms for automatic hypothesis formation concerning the number, location, orientation and shape of mitochondria and membranes. For this we construct a pattern theoretic model in the form of a prior probability measure on the space of configurations describing these hypotheses. This measure is synthesized by solving sequentially a jump–diffusion equation of generalized Langevin form. The jumps occur for the creation–annihilation of hypotheses, corresponding to a jump from one continuum to another in configuration (hypothesis) space. These continua (subhypotheses) are expressed in terms of products of low dimensional Lie groups acting on the generators of a template. We use a modified Bayes approach to obtain the hypothesis formation, also organized by solving a generalized Langevin equation. to justify this it is shown that the resulting jump‐diffusion process is ergodic so that the solution converges to the desired probability measure. to speed up the convergence we reduce the computation of the drift term in the stochastic differential equation analytically to a curvilinear integral, with the random term computed almost instantaneously. The algorithms thus obtained are implemented, both for mitochondria and membranes, on a 4000 processor parallel machine. Photographs of the graphics illustrate how automatic hypothesis formation is achieved. This approach is applied to deformable neuroanatomical atlases and tracking recognition from narrow band and high resolution sensor arrays.;},

keywords={Algorithms; Chromosomes; Complex systems; Geometric shapes; Inference; jump–diffusion random sampling; Landmarks; Markov processes; Mitochondria; Pattern theory; shape recognition; Statistics},

isbn={0035-9246},

language={English},

}
@Inbook{Robert2004,
author="Robert, Christian P.
and Casella, George",
title="The Metropolis---Hastings Algorithm",
bookTitle="Monte Carlo Statistical Methods",
year="2004",
publisher="Springer New York",
address="New York, NY",
pages="267--320",
abstract="This chapter is the first of a series on simulation methods based on Markov chains. However, it is a somewhat strange introduction because it contains a description of the most general algorithm of all. The next chapter (Chapter 8) concentrates on the more specific slice sampler, which then introduces the Gibbs sampler (Chapters 9 and 10), which, in turn, is a special case of the Metropolis--Hastings algorithm. (However, the Gibbs sampler is different in both fundamental methodology and historical motivation.)",
isbn="978-1-4757-4145-2",
doi="10.1007/978-1-4757-4145-2_7",
url="https://doi.org/10.1007/978-1-4757-4145-2_7"
}
@inbook{SMMM,
 author = "Marloes Maathuis, Mathias Drton, Steffen Lauritzen, Martin Wainwright",
 title = "Sequential Monte Carlo Methods",
 chapter = chapter7,
 volume = ,
 booktitle = "Handbook of Graphical Models",
 publisher = "CRC Press",
 year = 2018,
 doi = {10.1201/9780429463976-7},
 URL = "https://www.routledgehandbooks.com/doi/10.1201/9780429463976-7"
}
@misc{heine2022multilevel,
      title={Multilevel Bootstrap Particle Filter},
      author={Kari Heine and Daniel Burrows},
      year={2022},
      eprint={2104.08198},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}
@inproceedings{SGLD,
author = {Welling, Max and Teh, Yee Whye},
title = {Bayesian Learning via Stochastic Gradient Langevin Dynamics},
year = {2011},
isbn = {9781450306195},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an inbuilt protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a "sampling threshold" and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
pages = {681–688},
numpages = {8},
location = {Bellevue, Washington, USA},
series = {ICML'11}
}
@misc{hoffman2011nouturn,
      title={The No-U-Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo},
      author={Matthew D. Hoffman and Andrew Gelman},
      year={2011},
      eprint={1111.4246},
      archivePrefix={arXiv},
      primaryClass={stat.CO}
}

